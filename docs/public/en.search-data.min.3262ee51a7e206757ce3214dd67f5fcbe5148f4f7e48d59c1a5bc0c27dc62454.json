[{"id":0,"href":"/docs/reference/jobs/all/","title":"Common job methods","section":"Jobs","content":" Job.depends_on_file(filename) # Introduce a FileInvariant as dependency on Job.\nReturns a namedtuple (invariant, self), so you can continue your fluent call chain with either job.\nJob.depends_on_params(parameters) # Introduce a ParameterInvariant called \u0026lsquo;PI\u0026rsquo; + Job.job_id as dependency on Job.\nReturns a namedtuple (invariant, self), so you can continue your fluent call chain with either job.\nJob.depends_on_func(func, name): # Introduce a FunctionInvariant as dependency on Job.\nReturns a namedtuple (invariant, self), so you can continue your fluent call chain with either job.\n\u0026hellip;\n"},{"id":1,"href":"/docs/reference/jobs/filegeneratingjob/","title":"FileGeneratingJob","section":"Jobs","content":" pypipegraph2.FileGeneratingJob # A Job that creates a single file.\nppg2.FileGeneratingJob( job_id: str, callback_function: Callable[[Path], None], resources: Resources, depend_on_function: bool = True, empty_ok: bool = False, always_capture_output: bool = False ) -\u0026gt; None job_id -\u0026gt; The filename to create. Maybe a string or path callback_function takes a single parameter: The Path of the file to create. resources: See Resources. depend_on_function: Whether to create a FunctionInvariant for the callback_function. See FunctionInvariant. empty_ok: If it\u0026rsquo;s ok when the job creates an empty file. Otherwise an exception is raised and the job fails. always_capture_output - if True, the job\u0026rsquo;s stdout/stderr are stored on job.stdout/job.stderr. Otherwise they\u0026rsquo;re discarded in a successful run "},{"id":2,"href":"/docs/reference/jobs/multifilegeneratingjob/","title":"MultiFileGeneratingJob","section":"Jobs","content":" pypipegraph2.MultiFileGeneratingJob # A job that creates multiple files.\npypipegraph2.MultiFileGeneratingJob( files: List[Path], # todo: extend type attribute to allow mapping generating_function: Callable[List[Path]], resources: Resources = Resources.SingleCore, depend_on_function: bool = True, empty_ok=True, always_capture_output=True, ) files may be either a list of filenames, or a dictionary \u0026lsquo;friendly_key\u0026rsquo; -\u0026gt; filename.\nFilenames must be either str or pathlib.Path.\nThis is passed to the generating_function as a list/dictionary of Path-objects.\nIf a list, the order is preserved.\nFor the other parameters see FileGeneratingJob.\n"},{"id":3,"href":"/docs/reference/jobs/tempfilegeneratingjob/","title":"TempFileGeneratingJob","section":"Jobs","content":" pypipegraph2.TempFileGeneratingJob # A job that creates one file, which is deleted once all dependent jobs are done.\n(See the ephemeral jobs concept)\nIt is not removed if a dependant job fails.\nFor parameters see FileGeneratingJob.\n"},{"id":4,"href":"/docs/reference/jobs/multitempfilegeneratingjob/","title":"MultiTempFileGeneratingJob","section":"Jobs","content":" pypipegraph2.MultiTempFileGeneratingJob # A job that creates multiple files, which are deleted once all dependent jobs are done.\n(See the ephemeral jobs concept)\nIt is not removed if a dependant job fails.\nFor parameters see MultiFileGeneratingJob.\n\u0026ndash;\nIf you want to preserve some files, while treating others as temporary, you\u0026rsquo;ll need to chain another job that copies the files-to-be-kept somewher safe.\n"},{"id":5,"href":"/docs/reference/jobs/dataloadingjob/","title":"DataLoadingJob","section":"Jobs","content":"ppg2.DataLoadingJob\nppg2.DataLoadingJob( job_id, load_function, resources: Resources = Resources.SingleCore, depend_on_function=True, ): job_id: A job_id load_function: A parameter less function that stores data somewhere. resources: See Resources. depend_on_function: Whether to create a FunctionInvariant for the load_function. See FunctionInvariant. This is an ephemeral job, but there is no unloading.\nThis job runs in the controlling process (see process-structure).\nPrefer an AttributeLoadingJob if possible, since that can unload the data as well.\nThe load_function should return an object - it\u0026rsquo;s pickled representation is used to calculate the tracking hash of the job.\nIf you don\u0026rsquo;t return a value (= return None), this job can never invalidate it\u0026rsquo;s downstreams.\nIf you already have a hash handy (or your data is not pickleable), you can return a ppg2.ValuePlusHash(value, hash_hexdigest) object to circumvent the pickling requirement.\n"},{"id":6,"href":"/docs/reference/jobs/attributeloadingjob/","title":"AttributeLoadingJob","section":"Jobs","content":"ppg2.AttributeLoadingJob\nppg2.AttributeLoadingJob( job_id, obj, attr_name load_function, resources: Resources = Resources.SingleCore, depend_on_function=True, ): A DataLoadingJob that stores it\u0026rsquo;s results in obj.\u0026lt;attr_name\u0026gt;\nobj - the oject to store data in attr_name the attribute name to store the data in obj.\u0026lt;attr_name\u0026gt; For further parameters, see DataLoadingJob.\nAll the DataLoadingJob remarks apply here as well.\n"},{"id":7,"href":"/docs/reference/jobs/cacheddataloadingjob/","title":"CachedDataLoadingJob","section":"Jobs","content":"ppg2.CachedDataLoadingJob( cache_filename, calc_function, load_function, depend_on_function=True, resources: Resources = Resources.SingleCore, ): cache_filename - where to store the (pickled) output of the calc_function calc_function - a callback tha calculates the data to store load_function - a callback that takes one parameter - the (unpickled) calc function output, and assigns it somewhere in the current processs. See DataLoadingJob for the other parameters.\nCreates two jobs: One that calculates the data, and stores it in cache_filename (using pickle), and one that loads the data from cache_filename, and passes it to load_function\nThis function returns a namedtuple with two fields: .load \u0026amp; .calc for the two jobs created.\n"},{"id":8,"href":"/docs/reference/jobs/cachedattributeloadingjob/","title":"CachedAttributeLoadingJob","section":"Jobs","content":" ppg2.CachedAttributeLoadingJob # def CachedAttributeLoadingJob( cache_filename, object, attribute_name, data_function, depend_on_function=True, resources: Resources = Resources.SingleCore, ): Similar to CachedDataLoadingJob, but stores the data in an attribute of an object.\nUnlike the CachedDataLoadingJob you only need one function\ndata_function that returns the value to store \u0026amp; assign. See AttributeLoadingJob for the other parameters.\n"},{"id":9,"href":"/docs/reference/jobs/fileinvariant/","title":"FileInvariant","section":"Jobs","content":" ppg2.FileInvariant # ppg2.FileInvariant(filename: str | Path) An invariant that triggers if a file was changed.\nA file will only be rehashed, if it\u0026rsquo;s modification time or size have changed.\nChanging just the file time will not lead to an invalidation of downstream jobs.\nInstead of creating a FileInvariant(filename), you can also use job.depends_on_file(filename). See depends_on_file for details.\n"},{"id":10,"href":"/docs/reference/jobs/parameterinvariant/","title":"ParameterInvariant","section":"Jobs","content":" ppg2.ParameterInvariant # ppg2.ParameterInvariant(job_id, paramaters) A named job (\u0026lsquo;PI\u0026rsquo; + job_id) that triggers if the parameters have changed.\nParameters are \u0026lsquo;frozen\u0026rsquo; into a hash using DeepHash, which should work with all the pytohn value types.\nInstead of calling job.depends_on(ParameterInvariant(other_job.job_id, parameters)), you can also use other_job.depends_on_params(parameters). See depends_on_params for details.\n"},{"id":11,"href":"/docs/reference/jobs/functioninvariant/","title":"FunctionInvariant","section":"Jobs","content":" ppg2.FunctionInvariant # ppg2.FunctionInvariant(job_id, function) Function Invariants change their output hash if the function they cover changes.\nTechnically, they compare the python byte code (if you did not change the python version in between runs) or alternatively the source code (if you did).\nByte code is created using the dis module from the python\u0026rsquo;s standard library.\nInstead of calling job.depends_on(FunctionInvariant(name, func)), you can also use other_job.depends_on_func(parameters). See depends_on_params for details.\nNote that FunctionInvariant can also be called with just a function (in which case the name is automatically deduced, or with reversed parameters).\n"},{"id":12,"href":"/docs/reference/managing-pypipegraph/","title":"Managing Pypipegraph","section":"Reference","content":" pypipegraph2.new # def new( cores=reuse_last_or_default, run_mode=reuse_last_or_default, dir_config=reuse_last_or_default, log_level=reuse_last_or_default, allow_short_filenames=reuse_last_or_default, log_retention=reuse_last_or_default, cache_dir=reuse_last_or_default, prevent_absolute_paths=reuse_last_or_default, report_done_filter=reuse_last_or_default, ): cores - How many cores can the pipegraph use (default: all of them) run_mode - RunMode.CONSOLE | RunMode.NOTEBOOK | RunMode.NONINTERACTIVE dir_config - DirConfig(\u0026quot;.ppg\u0026quot;) - where are the runtime files stored (history, log, errors, \u0026hellip;) log_level - how much do we log (default: logging.INFO) allow_short_filenames - allow short (\u0026lt;= 3 letter) filenames (default: False) log_retention - how many runs of logfiles \u0026amp; errors do we keep (default 3) prevent_absolute_paths - prevent absolute paths in the pipeline (default: False) report_done_filter - (unused) The default for all values is \u0026lsquo;reuse_last_or_default\u0026rsquo; - so they start with default, but ppg.new(cores=4), followed by another ppg.new() will also use 4 cores.\nRunMode # RunMode.CONSOLE # In the default runmode (CONSOLE), you can not redefine jobs. You also get an interactive shell while the pipeline is running to interact with jobs (see interactive). Ctrl-C is disabled in this mode. Signal Hup is ignored, so the pipegraph does not end when the terminal is closed. (But you still need to do have your graph running in screen/tmux/dtach/(abduco to get be able to get it back).\nErrors / failed jobs are printed briefly to the console, and more detail is provided in a job specific logfile.\nRunMode.NOTEBOOK # In this mode you can redefine jobs (throwing away the former job definition, but keeping all links). There is no interactive shell. Ctrl-C aborts the pipegraph execution and kills all currently running jobs.\nRunMode.NONINTERACTIVE # For internal testing.\nDirConfig # The ppg by default writes logs to .ppg/log, history to .ppg/history and errors to .ppg/errors. Job output during job-runs is stored in .ppg/run (the run_dir). If you want to run multiple independend pipegraphs in one project, you can use DirConfig to overwrite all these at once with a new prefix directory. The DirConfig also allows you to overwrite each folder independently.\ndef DirConfig.__init__( self, root: Union[Path, str] = \u0026#34;.ppg\u0026#34;, log_dir: Union[Path, str, None, bool] = False, error_dir: Union[Path, str, None, bool] = False, history_dir: Union[Path, str, None, bool] = False, run_dir: Union[ Path, str, None, bool ] = False, # used by FileGeneratingJobs to e.g. recorde stdout cache_dir=\u0026#34;cache\u0026#34;, ) pypipegraph2.run # def pypipegraph2.run( print_failures=True, raise_on_job_error=True, event_timeout=1, dump_graphml=None, ): print_failures - if true, errors are logged with log_error (and show up on the console). If False, they\u0026rsquo;re log_debug (which may show up in your .ppg/logs.messages if you have set log_level to debug in ppg2.new().\nErrors are stored in per-job error logs either way. raise_on_job_error: Wether to raise an exception (after processing all jobs) if any job failed. If False, the result value of run is a dict of job_id -\u0026gt; RecordedJobOutcome. If True, this value is ppg.global_pipegraph.last_run_result event_timeout - (unused) dump_graphml - Dump the final structure of the user facing graph into .ppg/logs/latest/\u0026lt;log_file_name\u0026gt;).pre_prune.graphml. pypipegraph2.RecordedJobOutcome # An named tuple (outcome, error) showing the actual outcome of a job.\nThe outcome is o ppg2.JobOutcome, which is one of:\n.Success - Everything ok. .Skipped - this job did not need to run and was not run. .Failed - this job failed. Error message is in .error (on the RecordedJobOutcome) .UpstreamFailed - a job before this job failed and this job was not attempted. .Pruned - This job was pruned (created, but job.prune() was called), and therefore not run. pypipegraph2.global_pipegraph # The current pipegraph, created with pypipegraph2.new()\nA useful place to put \u0026lsquo;singletonizations\u0026rsquo; that work per graph.\n"},{"id":13,"href":"/docs/reference/managing-pypipegraph/interactive/","title":"Interactive","section":"Managing Pypipegraph","content":"Interactive\n"},{"id":14,"href":"/docs/reference/standalone-utilities/","title":"Standalone Utilities","section":"Reference","content":" ppg2_browse_graph_ml # An fzf based inspecter of graph_ml dumped with ppg2.run(dump_graphml)\nppg2_record_as_generated_file # (You are not going to need this)\n"},{"id":15,"href":"/docs/reference/utils/","title":"Utils","section":"Reference","content":" job_or_filename # Take a filename, or a job.\nReturn (Path(filename), dependency_for_that_file). The dependency might be a Job that creates the file, or a FileInvariant\nassert_uniqueness_of_object # assert that a given object of type(object) with a .name attribute is unique within this ppg.global_pipegraph lifetime.\nflatten_jobs # Take an arbitrary deeply nested list of lists of jobs and return just the jobs\npretty_log_errors # A decorator to capture exceptions and print really pretty tracebacks, for use outside of jobs.\nwrap_for_function_invariant # When you need to pass a function + parameters which ends up in a FunctionInvariant.\nThink a job with this function: lambda: some_func(filename)\nThis marks it so that the automatically generated FunctionInvariant tracks \u0026lsquo;somefunc\u0026rsquo;, not the lambda.\n"},{"id":16,"href":"/docs/faq/","title":"FAQ","section":"Docs","content":"If we had questions, we would add them here\n"},{"id":17,"href":"/docs/reference/jobs/resources/","title":"Resources enum","section":"Jobs","content":"What resources a job needs. Possible values:\nppg.Resources.SingleCore - Start as many as you have cores (see ppg.new) ppg.Resources.AllCores - all cores, but run one SingleCore job in parallel ppg.Resources.Exclusive - really use all cores. "}]